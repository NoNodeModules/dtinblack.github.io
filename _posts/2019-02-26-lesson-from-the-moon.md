---
layout: post
title: Can AI Learn From The Moon?
description: "Limits in the use of technology."
modified: 2019-02-26
tags: [Apollo 11, Neil Armstrong, Margaret Hamilton, Artificial Technology]
image:
  feature: features/lesson-from-the-moon.jpg
  credit: Universe Today
  creditlink: https://www.universetoday.com/13562/how-long-does-it-take-to-get-to-the-moon/
---

The Apollo 11 moon landing remains one of the greatest achievements in human endeavour and demonstrated what can be achieved through the combination of technology and people. But are there there any lessons for today's emerging technology that can be taken from a decades old project?

I remember watching the Apollo 11 moon landing on a black and white television and picking up on the excitement of the announcers reporting on every stage of the moon landing. Even today the whistle-beep sound punctuating communication  between the astronauts and Huston brings back memories of early July in 1969. Since then I have always been fascinated by the technology that carried three people: Neil Armstrong, Michael Collins and Edwin "Buzz" Aldrin, in something that seemed no bigger than a large skip over 235,000 miles, land them on the moon and then bring them safely back.

Winding the clock forward to today and we are abounding in new technology that is breaking into all areas of life from the production of food to medicine and transportation. But one - Artificial Intelligence - has caught the publics attention. Articles fill the newspapers and the web about AI and journalists, within a few paragraphs, link the eventual demise of human life from the technology with their imagination heightened by films such as [The Matrix](https://en.wikipedia.org/wiki/The_Matrix), [The Terminator](https://en.wikipedia.org/wiki/The_Terminator) and epitomised by the sinister computer HAL in [2001: A Space Odyssey](https://en.wikipedia.org/wiki/2001:_A_Space_Odyssey_(film)).

Artificial Intelligence is a technology that has been developing since the [mid 1940s](https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html) and started with the development of a mathematical model for a neuron: the basic unit of a human brain. Its development has accelerated in the last ten years by the increase in computing power for less cost, and the capability of gathering what feels like the near infinite amount of data. Artificial Intelligence uses the mathematical model of a neuron in a simplistic way to simulate a neuron in the brain and when connected together in a network it is used to "learn" by analysing data. For example, being able to distinguish between the pictures of cats and dogs. But AI can neither explain nor do they understand which are some of the key characteristics of being human. Also, the largest number of neurons used in the current applications of Artificial Intelligence are in the millions whereas the best estimate for the neurons in a human brain is 100 billion or 100,000,000,000. Even if a computer could handle the same number of neurons as a human brain then how they connect to each other to create the wide range of human characteristics would propbably remain a mystery for a very long time. However, there are a number of applications of Artificial Intelligence that are in everyday use. For example, communicating with people, in systems such as [Siri](https://en.wikipedia.org/wiki/Siri) and [Alexa](https://www.forbes.com/sites/bernardmarr/2018/10/05/how-does-amazons-alexa-really-work/#423da0e41937) and guiding our purchase of goods and services with, [Amazon](https://www.forbes.com/sites/blakemorgan/2018/07/16/how-amazon-has-re-organized-around-artificial-intelligence-and-machine-learning/#27fbf3173618) and [Netflix](https://blogs.nvidia.com/blog/2018/06/01/how-netflix-uses-ai/). I am sure that technology will continue to develop and become a greater part of our lives, and the concerns about acting more like a human are starting to be researched in projects such as DAPRA's project: [Building Trusted Human-Machine Partnerships](https://www.darpa.mil/news-events/2019-01-31)

Behind the Apollo 11 moon landing there were thousands and thousands of people involved who covered a wide range of skills. But there was one area that fascinated me most which was the design of the software that kept the space craft on its flight path and safely landed the astronauts on the moon. But even using state of the art software there were two potentially serious events that could have ended in tragedy. The first event was the [computer overloaded](https://www.youtube.com/watch?v=Qqe7-rFRrkc) with extraneous data just as they were about to land and the mission could have been aborted.   The ground controllers and Neil Armstrong knew that by restarting the computer it would would keep the essential programs running for the landing and the mission continued. However, a second problem started to loom. Neil Armstrong's attention was drawn by the appearance of a rocky crater that the guidance system was taking them too. It was to late to retarget the computer for a different trajectory. Instead, he took over the landing from the computer and manoeuvred across the crater and landed at [Tranquility Base](https://www.youtube.com/watch?v=Qqe7-rFRrkc).

What can be learned form the moon landing? Without the intervention of the astronauts the mission would either of been aborted or ended in tragedy.  Without the technology the astronauts would not have reached the moon. One depended on the other. Therefore it is important to keep people at the centre of the technology and that they fully understand its limits. Artificial Intelligence will bring many benefits but only if it enhances people's capabilities rather than replace them!
